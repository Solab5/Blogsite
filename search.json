[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I‚Äôm Morris Twinomugisha . I‚Äôm a machine learning engineer who loves building data-science and developer tools üë∑üèº‚Äç‚ôÇÔ∏è. I‚Äô‚Äôm as well a stint in management consulting."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nMakerere University, Kampala | UGANDA Bachelors Degree in Statistics | Aug 2019 - July 2022\nSt Mary‚Äôs College Rushoroza | Kabale, UG Uganda Advanced Certificate of Education | Jan 2017 - Nov 2018"
  },
  {
    "objectID": "about.html#trainings",
    "href": "about.html#trainings",
    "title": "About",
    "section": "Trainings",
    "text": "Trainings\nPractical Deep Learning for coders | Fast.ai\nApplied DataScience | WorldQuant University"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nDeep Conclusions | Data Scientist | November 2022 - present\nSolab Associates | Data Analyst - Internship|"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogsite",
    "section": "",
    "text": "Exploring the Secrets of Stable Diffussion\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\nMorris Twinomugisha\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Secrets of Stable Diffussion Part Two\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\nMorris Twinomugisha\n\n\n\n\n\n\n  \n\n\n\n\nHave you ever wondered how websites recommend products for you!\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nMorris Twinomugisha\n\n\n\n\n\n\n  \n\n\n\n\nWhat is it really\n\n\n\n\n\n\n\nposts\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nMorris Twinomugisha\n\n\n\n\n\n\n  \n\n\n\n\nThe truth about Deep Learning\n\n\n\n\n\n\n\nposts\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2022\n\n\nMorris Twinomugisha\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "The truth about Deep Learning",
    "section": "",
    "text": "Awesome deep learning\n\nYeah, I mean it ‚ÄúAwesome!‚Äù. Deep learning is the most awesome, exciting, and interesting stuff I feel every data scientist should brag about. When I started my data science journey, I thought Learning basic machine learning was all until I heard about things like neural networks, NLP, Computer vision, and so forth. I was so eager to learn how deep learning works. Along the journey, I realized every business or institution needs artificial intelligence in all their support systems other than staying in the comfort zone of traditional approaches to problem-solving and innovation.\n\nHowever, getting started can be hectic and overwhelming since resources are all over the web, and getting the right resources can be difficult unless you have someone already practicing it to guide you, else you can waste a lot of time without getting started.\nYou can imagine what I went through to get started. If you are passionate about deep learning and don‚Äôt know where to start, or maybe you‚Äôve started but feel you aren‚Äôt understanding anything from some random YouTube videos or resources. This short article is meant for you. Allow me to share with you a few of the facts and resources you need to know about deep learning maybe you could consider making a decision you will never regret today. Deep learning has proved to be the best machine learning technique in different areas all over the world including areas like Natural Language Processing (NLP), Computer Vision, Medicine, Biology, Image generation/enhancement, Recommendation systems, Playing games, Robotics, Other applications ‚Äì financial and logistical forecasting; text-to-speech; much more.\nMany experts will say that to get started with deep learning, you need a lot of math, lots of data, lots of expensive computers, or maybe a Ph.D.¬†Trust me anyone irrespective of any field can practice deep learning. As long as you have an internet connection, you can use many online platforms to run deep learning codes on any computer. A commonly used one is a google Collab notebook. For math, simple high school math is sufficient for anyone to get started. The required basic Linear algebra and calculus can be understood along the way.\nIf you‚Äôve done some basic machine learning before, the approach to deep learning is almost the same. This however doesn‚Äôt imply that deep learning is only for those that have done machine learning before, I have seen experts that have progressed with deep learning minus prior machine learning experience. A critique of basic machine learning is that it mainly involves dealing with quantitative data, unlike most deep learning problems.\nDeep learning problems are mostly texts from the web, sound, pictures, and videos among others thus there is always a need to train models that can enable transforming input data into numeric. This is because computers can only understand numbers.\nAll these applications stipulate why every business or institution needs artificial intelligence in all their support systems other than staying in the comfort zone of traditional approaches to problem-solving and innovation. If this has motivated you, you can check on the following resources which I believe can be a very mounting stone for anyone that would like to embark on a new journey of deep learning.\n\nlink to deep learning for coders(by Jeremy Howard and Sylvain Gugger)\nlink to MIT course for deep learning(MIT)\nLink to Coursera deep learning course(by Andrew Ng)"
  },
  {
    "objectID": "posts/blog2/index.html#what-is-it-really",
    "href": "posts/blog2/index.html#what-is-it-really",
    "title": "What is it really",
    "section": "WHAT IS IT REALLY?",
    "text": "WHAT IS IT REALLY?\nIn 2015 the idea of creating a computer system that could recognise images was considered so outrageously challenging that it was the basis of this XKCD joke:\n\nIn this tutorial we are going to create a computer vision model that can classify whether an image is a car, plane, train or bicycle. However this approach can work for any computer vision problem.\nBy the end of this tutorial, you will be able to implemenent an image classification model in just a few minutes, using entirely free resources!. We shall be using the fastai library A high framework library built on top of pytorch. Feel free to see the ducumentation here for more understanding.\nThe basic steps we‚Äôll take are:\n\nUse DuckDuckGo to search for images of car, plane, train or bicycle photos.\nFine-tune a pretrained neural network to recognise the groups\nTry running this model on a picture of a plane and see if it works."
  },
  {
    "objectID": "posts/blog2/index.html#step-1-download-images-of-plane-train-car-and-bicycles.",
    "href": "posts/blog2/index.html#step-1-download-images-of-plane-train-car-and-bicycles.",
    "title": "What is it really",
    "section": "Step 1: Download images of plane, train, car and bicycles.",
    "text": "Step 1: Download images of plane, train, car and bicycles.\nThe first thing we are going to do is to install the latest versions of fastai and duckduckgo_search since we shall be using it to search images on the internet.\n\n!pip install -Uqq fastai duckduckgo_search\n\nWe create a search_images function that we shall use to iterate on the different terms we shall be searching on the web.\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=100):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nLet‚Äôs start by searching for a plane photo and see what kind of result we get. We‚Äôll start by getting URLs from a search:\n\n#NB: `search_images` depends on duckduckgo.com, which doesn't always return correct responses.\n#    If you get a JSON error, just try running it again (it may take a couple of tries).\nurls = search_images('plane photos', max_images=1)\nurls[0]\n\nSearching for 'plane photos'\n\n\n'http://airplanes.itsabouttravelling.com/wp-content/uploads/2020/02/c-fjzs-air-canada-boeing-777-300er-02-scaled.jpg'\n\n\n‚Ä¶and then download a URL and take a look at it:\n\nfrom fastdownload import download_url\ndest = 'plane.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\nNow let‚Äôs do the same with ‚Äúcar photos‚Äù and see if it works as well:\n\ndownload_url(search_images('car photos', max_images=1)[0], 'car.jpg', show_progress=False)\nImage.open('car.jpg').to_thumb(256,256)\n\nSearching for 'car photos'\n\n\n\n\n\nOur searches seem to be giving reasonable results, so let‚Äôs grab a few examples of each of ‚Äúplane‚Äù, ‚Äúcar‚Äù, ‚Äútrain‚Äù and ‚Äúbicycle‚Äù photos, and save each group of photos to a different folder:\n\nvehicles = 'car','train', 'bicycle', 'plane'\npath = Path('vehicles')\n\nfor o in vehicles:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'car photo'\nSearching for 'train photo'\nSearching for 'bicycle photo'\nSearching for 'plane photo'"
  },
  {
    "objectID": "posts/blog2/index.html#step-2-train-our-model",
    "href": "posts/blog2/index.html#step-2-train-our-model",
    "title": "What is it really",
    "section": "Step 2: Train our model",
    "text": "Step 2: Train our model\nSome photos might not download correctly which could cause our model training to fail, so we‚Äôll remove them:\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n17\n\n\nTo train a model, we‚Äôll need DataLoaders, which is an object that contains a training set (the images used to create a model) and a validation set (the images used to check the accuracy of a model ‚Äì not used during training). In fastai we can create that easily using a DataBlock, and view sample images from it:\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\nHere what each of the DataBlock parameters means:\nblocks=(ImageBlock, CategoryBlock),\nThe inputs to our model are images, and the outputs are categories (in this case, ‚Äúplane‚Äù, ‚Äúcar‚Äù, ‚Äútrain‚Äù or ‚Äúbicycle‚Äù).\nget_items=get_image_files, \nTo find all the inputs to our model, run the get_image_files function (which returns a list of all image files in a path).\nsplitter=RandomSplitter(valid_pct=0.2, seed=42),\nSplit the data into training and validation sets randomly, using 20% of the data for the validation set.\nget_y=parent_label,\nThe labels (y values) is the name of the parent of each file (i.e.¬†the name of the folder they‚Äôre in, which will be pane, car, train, or bicycle).\nitem_tfms=[Resize(192, method='squish')]\nBefore training, resize each image to 192x192 pixels by ‚Äúsquishing‚Äù it (as opposed to cropping it).\nNow we‚Äôre ready to train our model. The fastest widely used computer vision model is resnet18. You can train this in a few minutes, even on a CPU! (On a GPU, it generally takes under 20 seconds‚Ä¶)\nfastai comes with a helpful fine_tune() method which automatically uses best practices for fine tuning a pre-trained model, so we‚Äôll use that.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.820234\n      0.014370\n      0.000000\n      00:09\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.037691\n      0.001882\n      0.000000\n      00:05\n    \n    \n      1\n      0.032922\n      0.003439\n      0.000000\n      00:04\n    \n    \n      2\n      0.022986\n      0.003907\n      0.000000\n      00:04\n    \n  \n\n\n\nGenerally when I run this I see 100% accuracy on the validation set (although it might vary a bit from run to run).\n‚ÄúFine-tuning‚Äù a model means that we‚Äôre starting with a model someone else has trained using some other dataset (called the pretrained model), and adjusting the weights a little bit so that the model learns to recognise your particular dataset. In this case, the pretrained model was trained to recognise photos in imagenet, and widely-used computer vision dataset with images covering 1000 categories).\nAnd another interesting thing to look at when dealing with categorical target variables is that you can run a confusion matrix which gives you a visual representation and a sense on what classes are hard to predict. For our case, there is no time any class is being predicted wrongly which means that our model is performing very well.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "posts/blog2/index.html#step-3-use-our-model",
    "href": "posts/blog2/index.html#step-3-use-our-model",
    "title": "What is it really",
    "section": "Step 3: Use our model",
    "text": "Step 3: Use our model\nLet‚Äôs see what our model thinks about that car we downloaded at the start:\n\npred,_,probs = learn.predict(PILImage.create('car.jpg'))\n\n\n\n\n\n\n\n\n\nprint(f\"This is a: {pred}.\")\nprint(f\"Probability it's a car: {probs[1]:.4f}\")\n\nThis is a: car.\nProbability it's a car: 1.0000\n\n\nWe can then pass export to our learner to save our model as a pickel file. This means that any one now can use our model for making predictions\n\nlearn.export('model.pkl')\n\nGood job, resnet18. :)\nSo, as you see, in the space of a few years, creating computer vision classification models has gone from ‚Äúso hard it‚Äôs a joke‚Äù to ‚Äútrivially easy and free‚Äù!\nIt‚Äôs not just in computer vision. Thanks to deep learning, computers can now do many things which seemed impossible just a few years ago, including creating amazing artworks, and explaining jokes. It‚Äôs moving so fast that even experts in the field have trouble predicting how it‚Äôs going to impact society in the coming years.\nOne thing is clear ‚Äì it‚Äôs important that we all do our best to understand this technology, because otherwise we‚Äôll get left behind!"
  },
  {
    "objectID": "posts/blog3/index.html#latent-factors",
    "href": "posts/blog3/index.html#latent-factors",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Latent factors",
    "text": "Latent factors\nThe idea behind collaborative filtering is latent factors. Latent factors make it possible for a model to tell what product you may like and these kinds of factors can be learned based on what other users like.\n\njargon: Latent factors are factors that are important for the prediction of the recommendations, but are not explicitly given to the model and instead learned."
  },
  {
    "objectID": "posts/blog3/index.html#learning-the-factors",
    "href": "posts/blog3/index.html#learning-the-factors",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Learning the factors",
    "text": "Learning the factors\nStep 1: Randomly initialize some parameters. The left matrix represents the embedding matrix for the the userid whereas the upper matrix is for the movieid and all these parameters are randomly initialised. For both cases, blue represents the biases. For ourcase each id (wether movieid or userid) the number of latent factors are the same. (for our case each id has 5 factors and the extra 6th (bias)).\n\njargon: Embedding matrix is what we multiply and embedding with, and in this case collaborative filtering problem is learned through training.\n\n\n\n\nlat\n\n\nStep 2: Calculate the predictions. Just think of a simple math problem \\(y=mx+c\\) where we have two changing parameters m and c where c is the bias term. Our bias works in the same way. We use dot product to multiply user factors and movies factors the add both the biases. This is how we obtain the values in white space.\n\njargon: Dot product is when you multiply the corresponding elements of two vectors and add them up\n\nStep 3: The next step is to calculate the loss. We can use any loss function as long as it can be optimized. Remember at each step we compare the predicted value and then compare it with the actual value then used use stochastic gradient descent to make the parameters as better as possible. In this tutorial we use mean squared error since it works pretty fine. Other loss functions like mean absolute error can work perfect as well"
  },
  {
    "objectID": "posts/blog3/index.html#creating-dataloaders",
    "href": "posts/blog3/index.html#creating-dataloaders",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\nRemember when we listed what was in our path, there were many things including the moivie names in u.item, I think it is better if we use this than using the ids. This makes our dataframe more human friendly. We can use pandas again to do this.\n\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nWe can then use merge to combine our movies dataframe with the ratings\n\nratings = ratings.merge(movies)\n\n\nratings.shape\n\n(100000, 5)\n\n\nAnd wow, our dataframe looks nice, We can then build a DataLoaders object from this table. By default, it takes the first column for the user, the second column for the item (here our movies), and the third column for the ratings. We need to change the value of item_name in our case to use the title instead of the IDs:\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      542\n      My Left Foot (1989)\n      4\n    \n    \n      1\n      422\n      Event Horizon (1997)\n      3\n    \n    \n      2\n      311\n      African Queen, The (1951)\n      4\n    \n    \n      3\n      595\n      Face/Off (1997)\n      4\n    \n    \n      4\n      617\n      Evil Dead II (1987)\n      1\n    \n    \n      5\n      158\n      Jurassic Park (1993)\n      5\n    \n    \n      6\n      836\n      Chasing Amy (1997)\n      3\n    \n    \n      7\n      474\n      Emma (1996)\n      3\n    \n    \n      8\n      466\n      Jackie Chan's First Strike (1996)\n      3\n    \n    \n      9\n      554\n      Scream (1996)\n      3\n    \n  \n\n\n\nTo represent collaborative filtering in PyTorch we can‚Äôt just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:\n\nn_users  = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nWe can check the shape to ensure everything is running perfect\n\nuser_factors.shape, movie_factors.shape\n\n(torch.Size([944, 5]), torch.Size([1665, 5]))\n\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.\nFortunately, it turns out that we can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors."
  },
  {
    "objectID": "posts/blog3/index.html#collaborative-filtering-from-scratch",
    "href": "posts/blog3/index.html#collaborative-filtering-from-scratch",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Collaborative Filtering from Scratch",
    "text": "Collaborative Filtering from Scratch\nPyTorch already provides a Module class, which provides some basic foundations that we want to build on. We just add the name of this superclass after the name of the class that we are defining.\nThe final thing that you need to know to create a new PyTorch module is that when your module is called, PyTorch will call a method in your class called forward, and will pass along to that any parameters that are included in the call. Here is the class defining our dot product model:\nIf you haven‚Äôt seen object-oriented programming before, it can be a little bit intimidating but then don‚Äôt worry, its a few lines of code and can be easy to read when you do it line by line. But dont hesitate to google some tutorial and have a sense how object oriented programming works.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n\nWe now have our architecture ready, Let‚Äôs train and see how everything goes\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.958848\n      0.956008\n      00:10\n    \n    \n      1\n      0.872096\n      0.876626\n      00:10\n    \n    \n      2\n      0.747695\n      0.839320\n      00:10\n    \n    \n      3\n      0.600863\n      0.822649\n      00:10\n    \n    \n      4\n      0.487104\n      0.822984\n      00:10\n    \n  \n\n\n\nAnd wow, this looks fine with just 5 epochs. You can try iterating more and changing the learning rate to see wether you can make this better. The wd stands for weight decay but most deeplearning practioners call it L2 regularization. So it‚Äôs better to understand such jargon since thats what most reseach papers use.\nActually we used pytorch‚Äôs embedding previously to initialize our parameters, we can decide not to do that and build our own. Let‚Äôs do that. We begin by creating a function that takes size as an argument and then returns a tensor if randomly initialized parameters.\n\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\nWe can then pass in the function instead of pytorch‚Äôs embedding.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n\nLet‚Äôs train again.\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.944513\n      0.958574\n      00:11\n    \n    \n      1\n      0.847001\n      0.873356\n      00:10\n    \n    \n      2\n      0.736139\n      0.838051\n      00:11\n    \n    \n      3\n      0.584478\n      0.821091\n      00:11\n    \n    \n      4\n      0.484529\n      0.821726\n      00:10\n    \n  \n\n\n\nAnd wow, it even looks a little bit better than using pytorch‚Äôs embedding. This shows you the power of doing things from scratch."
  },
  {
    "objectID": "posts/blog3/index.html#interpreting-embeddings-and-biases",
    "href": "posts/blog3/index.html#interpreting-embeddings-and-biases",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\nTo be an expert deep learning practioner, you need to be able to interprete models, there are some insights you can‚Äôt get from the data till you first run the models. Forexample let‚Äôs try interpreting the bias. We are using argsort to sort the values in ascending order and return the first five indices of those values. When we index them onto the title class, we get their names and print them.\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Robocop 3 (1993)',\n 'Home Alone 3 (1997)',\n 'Mortal Kombat: Annihilation (1997)']\n\n\nThe interpretation for this is that let‚Äôs say Robocop in this list is an action movie (I am not sure of it being an action movie). People that like movies may not like it as well. This implys that this particular movie won‚Äôt be recommended to users.\nWe can do the same for those movies with a high bias by passing True to descending. This prints for us movies that are liked. Forexample say Titanic, a user might like it even if he or she doesn‚Äôt like romantic movies. So such a movie will be highly recommended for users to watch.\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n 'Shawshank Redemption, The (1994)',\n \"Schindler's List (1993)\",\n 'Titanic (1997)',\n 'Silence of the Lambs, The (1991)']\n\n\nWe can also do an interpretation on the embeddings, we have 50 factors and representing them on a 2D graph isnot possible. We can use PCA to just reduce the factors two just two dimensions. Surprisingly similars will tend to be clustered near each other, this is something that we didnt program. We didnt even do any feature engineering but the model does this for us. And as I said, these are insights you cannot get from the data untill you run the models.\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\nAll this that we‚Äôve written from from scratch can be done using a fastai‚Äôs framwork collab_learner. Let‚Äôs try it and see whether we get similar results."
  },
  {
    "objectID": "posts/blog3/index.html#using-fastai-collab-learner",
    "href": "posts/blog3/index.html#using-fastai-collab-learner",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Using fastai Collab-Learner",
    "text": "Using fastai Collab-Learner\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.962636\n      0.950697\n      00:10\n    \n    \n      1\n      0.851842\n      0.885249\n      00:12\n    \n    \n      2\n      0.757142\n      0.839390\n      00:11\n    \n    \n      3\n      0.572357\n      0.825587\n      00:11\n    \n    \n      4\n      0.490504\n      0.826604\n      00:11\n    \n  \n\n\n\nWe can also look into our model,\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nAnd surprisingly, when we check the movie with maximum bias weights. We get the same as our model built from scratch.\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n 'Silence of the Lambs, The (1991)',\n 'Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Star Wars (1977)']\n\n\nThese days the cool kids are using deep-learning. Let‚Äôs try it out and see how it works as well."
  },
  {
    "objectID": "posts/blog3/index.html#deep-learning",
    "href": "posts/blog3/index.html#deep-learning",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Deep learning",
    "text": "Deep learning\nFirstai has a function get_emb_sz that recommends for as sizes to use for the factors, however you can use different sizes other than this.\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nAnd like other deep learning frameworks, we need a sequential layer That has both linear and ReLU activations that we pass in our data. In the forward function, we are concatenating the embeddngs for both the users and the movies and this is actually what we pass to the sequential layer. Then pytorch will calculate gradients for us and stochastic gradient descent as well.\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n\nWe can train as usual.\n\nmodel = CollabNN(*embs)\n\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.921936\n      0.952098\n      00:12\n    \n    \n      1\n      0.868925\n      0.927091\n      00:10\n    \n    \n      2\n      0.885512\n      0.885665\n      00:11\n    \n    \n      3\n      0.850595\n      0.873478\n      00:11\n    \n    \n      4\n      0.761019\n      0.877106\n      00:11\n    \n  \n\n\n\nSurprisingly all our previous models beat the deep learning approach. This shows us why deep-learning shouldn‚Äôt always be the first priority. At-times simpler models can work just perfect.\nThank you, Hope this tutorial will be helpful."
  },
  {
    "objectID": "posts/blog4/index.html",
    "href": "posts/blog4/index.html",
    "title": "Exploring the Secrets of Stable Diffussion",
    "section": "",
    "text": "Hi everyone! I‚Äôm super excited to share with you my journey into the world of generative AI and diffusion models. These are amazing technologies that can create realistic images from text prompts. You might have seen some examples on twitter or reddit, where people type a word and get an image of it. Well, in this blog / tutorial, I‚Äôm going to show you how to do that yourself.\nThis tutorial has two parts: first, we‚Äôll use huggingface‚Äôs pipeline, which is very cool and easy to use. Then, we‚Äôll build our own pipeline from scratch, so we can have more control over the code."
  },
  {
    "objectID": "posts/blog4/index.html#classifier-free-guidance-cfg",
    "href": "posts/blog4/index.html#classifier-free-guidance-cfg",
    "title": "Exploring the Secrets of Stable Diffussion",
    "section": "Classifier free guidance CFG",
    "text": "Classifier free guidance CFG\nGenerating new text from scratch is a challenging task, so sometimes the model might produce something that does not match your expectations. That‚Äôs why CFG gives you the power to control the inference step and steer the output in the direction you want. Isn‚Äôt that amazing?\n\nNegative Prompt\nImagine you have created a cute puppy with our pipeline. But wait, it has a blue top! What if you don‚Äôt like blue and want to avoid it in your image? No problem, our pipeline has a solution for that. Just use the negative_prompt argument and specify blue. The model will do its best to remove any blue from your image. Isn‚Äôt that amazing?\n\ntorch.manual_seed(12340)\npipe(prompt, negative_prompt=\"blue\").images[0]\n\n\n\n\n\n\n\n\n\nImage2Image\nYou might think that the pipeline can only do text-to-image, but it can do much more! One of the amazing features is image2image. This means you can give it an image and ask it to transform it into another image based on your description. For example, you can turn a sketch into a realistic painting, or a photo into a cartoon. Let me show you how to do this.\nThe code below is not very complicated, but the main idea is that it defines a function that takes a list of images, a number of rows and columns, and returns a grid of these images arranged in the specified way. This is useful for displaying the output images side by side.\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nLet‚Äôs now import the Image 2 Image pipeline as follows\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nWe can download an image to use as a starting image.\n\n!curl --output macaw.jpg 'https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png'\ninput_image = Image.open(\"macaw.jpg\").convert(\"RGB\")\ninput_image\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 46150  100 46150    0     0   385k      0 --:--:-- --:--:-- --:--:--  385k\n\n\n\n\n\nIam excited to show you how to create amazing art from a simple sketch! Look at this image: it‚Äôs just a hand-drawn outline of a scene. But we can use our model to fill in the details and make it look realistic. How? Let‚Äôs find out together! Follow these steps to instantiate our model and get ready to generate some awesome art!\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",revision=\"fp16\",torch_dtype=torch.float16,).to(\"cuda\")\n\n\n\n\nWow! Look at the amazing results of our image processing. The images we got are so fascinating and they perfectly match our original image. Isn‚Äôt that awesome?\n\ntorch.manual_seed(1000)\nprompt = \"wolf howling at the moon, photorealistic 4K\"\nimages = pipe(prompt=prompt, num_images_per_prompt=3, image=input_image, strength=0.8, num_inference_steps=50).images\nimage_grid(images, rows=1, cols=3)\n\n\n\n\n\n\n\nI‚Äôm excited to share with you some of the amazing features of diffusion models. They are not just simple tools, but powerful methods to create realistic and diverse images. You can use some free tips to guide the process and get the results you want. For example, you can try:\n\nTextual Invation\nCallbacks\n\nThese are great ways to practice and master the pipeline.\nThis is just a glimpse of what diffusion models can do and how they work. If you are curious and eager to learn more, join me in part two here"
  },
  {
    "objectID": "posts/blog5/index.html",
    "href": "posts/blog5/index.html",
    "title": "Exploring the Secrets of Stable Diffussion Part Two",
    "section": "",
    "text": "You can checkout part one here since it‚Äôs the foundation for this part two.\n\nWelcome back to part 2 of this amazing blog! You are awesome for continuing your learning journey with me. In this part, we will dive deep into the fascinating world of stable diffusion models and how they work. Are you ready? Let‚Äôs go!\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport logging\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom fastcore.all import concat\nfrom huggingface_hub import notebook_login\nfrom PIL import Image\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers import LMSDiscreteScheduler\nfrom tqdm.auto import tqdm\n\nlogging.disable(logging.WARNING)\n\ntorch.manual_seed(1)\nif not (Path.home()/'.huggingface'/'token').exists(): notebook_login()\n\n\n\n\n\nfrom PIL import Image\nfrom torchvision import transforms as tfms\nfrom IPython.display import display\n\n\n\n\nDiffusion models are amazing! They can create realistic data from random noise by undoing the noise. For example, an image model can start with a noisy image and end up with a natural image.\nImagine you have a noisy picture of a cat just like above and you want to see the cat clearly. How can you do that? One way is to use a diffusion model for generative image denoising. This is a method that tries to estimate the amount of noise in the image and then subtract it from the noisy image. By doing this repeatedly and updating the model‚Äôs parameters, you can get a clean image of the cat as the output. This is how diffusion models can help you recover images from noise.\nBut this process can be slow, so we use stable diffusion. It makes the image smaller, diffuses it, and then makes it bigger again. If you have enough compute resources like those of google, you can just use diffusion without compressing the image size. Let‚Äôs see how stable diffusion works.\n\n\n\nLike we said, instead of noising and denoising on the actual image, we compress the image to a smaller representation and do all the diffusion processes on the smaller version of the image.\nStable diffusion models have three main components:\n\nAn autoencoder\nA U-net\nA text encoder\n\nYou can use pretrained models for these components, which saves you a lot of time and computing power. You can even run this code on a free Google Colab notebook with a 16GB GPU. Let‚Äôs inspect all these pieces separately.\n\n\nYou‚Äôre about to learn something amazing: how an autoencoder works! Don‚Äôt worry if you don‚Äôt know what a transformer model is, you don‚Äôt need it for this. An autoencoder has two parts: an encoder and a decoder. The encoder takes some words and turns them into a vector, which is like a list of numbers. The decoder takes that vector and turns it back into words. This way, we can compress and reconstruct information with our model.\n\n\n\n\nWe love the u-net! It makes our image resizing easy and fast. It takes our image and squeezes it into a tiny representation. This helps us speed up our diffusion process and get high-quality images. Then it expands the results into a full-sized image.\n\n\n\nThe text encoder is the one the helps to match our prompt with a corresponding image. For this case we use CLIP which has been trained on a variety of images and can match almost every prompt with a corresponding image.\n\n\nLet‚Äôs get started with the work by grabbing all the goodies we need, including the models. We can instantiate them easily, but we need to specify the repo where they are stored so that we can download them. They are not the defaults, so feel free to pick any of them from the hub. There are plenty of options to choose from.\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\nbeta_start,beta_end = 0.00085,0.012\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is the text we will use to create an image with our model. We can ask it to make anything we want, even something silly like cows riding bikes. We also set some parameters to control the size, quality, and similarity of the image. We only want one image for now, so we use a batch size of one.\n\nprompt = [\"a photograph of an astronaut riding a cow\"]\n\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1\n\nOur prompt is a text that we want the model to process. But the model does not understand text, it only understands numbers. So we need to convert our text into numbers. That‚Äôs where the tokenizer comes in handy. The tokenizer is a tool that we imported from the hub. It can transform any text into a sequence of numbers that the model can work with. We use the tokenizer on our prompt to get its numeric representation.\n\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\nWow! Check out the outputs! Our text has been transformed into numbers!\n\ntext_input['input_ids']\n\ntensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  9706, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n\n\nWe can easily see the matching words in our sentences with a simple loop. Isn‚Äôt that amazing?\n\nfor t in text_input[\"input_ids\"][0][:9]: print(t,tokenizer.decoder.get(int(t)))\n\ntensor(49406) <|startoftext|>\ntensor(320) a</w>\ntensor(8853) photograph</w>\ntensor(539) of</w>\ntensor(550) an</w>\ntensor(18376) astronaut</w>\ntensor(6765) riding</w>\ntensor(320) a</w>\ntensor(9706) cow</w>\n\n\nNext, we apply a text encoder to transform our tokens into a vector embedding. This is a way of representing the meaning of our text in a numerical form. We can see how big this vector is by looking at its size.\n\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nWe want to create something new, not just copy an image from the web. That‚Äôs why we also encode an empty string along with our input. This way, our model can generate a unique output. It‚Äôs the same process as before, but with more creativity!\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\nuncond_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nWe then combine the two embeddings: one that represents our text and another that represents an empty string. Then we use a special function to merge them together and get a single embedding that captures the essence of our text. This way, we can compare different texts based on their embeddings and find the most similar ones.\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\nWe are ready to make some random noise for our model! We use half precision to speed up the inference and move the data to the GPU. The shape of the latents is smaller than the images because we want to compress them efficiently.\n\ntorch.manual_seed(100)\nlatents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\nlatents = latents.to(\"cuda\").half()\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nAs we explained, timesteps control the noise level in the image. The lower the timesteps, the higher the noise and vice versa. So we use the value we defined earlier, which is 70.\n\nscheduler.set_timesteps(num_inference_steps)\n\n\nlatents = latents * scheduler.init_noise_sigma\n\n\nscheduler.timesteps\n\ntensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304,\n        897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826,\n        796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348,\n        694.9565, 680.4783, 666.0000, 651.5217, 637.0435, 622.5652, 608.0870,\n        593.6087, 579.1304, 564.6522, 550.1739, 535.6957, 521.2174, 506.7391,\n        492.2609, 477.7826, 463.3043, 448.8261, 434.3478, 419.8696, 405.3913,\n        390.9130, 376.4348, 361.9565, 347.4783, 333.0000, 318.5217, 304.0435,\n        289.5652, 275.0870, 260.6087, 246.1304, 231.6522, 217.1739, 202.6957,\n        188.2174, 173.7391, 159.2609, 144.7826, 130.3043, 115.8261, 101.3478,\n         86.8696,  72.3913,  57.9130,  43.4348,  28.9565,  14.4783,   0.0000],\n       dtype=torch.float64)\n\n\n\nscheduler.sigmas\n\ntensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301,  9.6279,  8.9020,  8.2443,\n         7.6472,  7.1044,  6.6102,  6.1594,  5.7476,  5.3709,  5.0258,  4.7090,\n         4.4178,  4.1497,  3.9026,  3.6744,  3.4634,  3.2680,  3.0867,  2.9183,\n         2.7616,  2.6157,  2.4794,  2.3521,  2.2330,  2.1213,  2.0165,  1.9180,\n         1.8252,  1.7378,  1.6552,  1.5771,  1.5031,  1.4330,  1.3664,  1.3030,\n         1.2427,  1.1852,  1.1302,  1.0776,  1.0272,  0.9788,  0.9324,  0.8876,\n         0.8445,  0.8029,  0.7626,  0.7236,  0.6858,  0.6490,  0.6131,  0.5781,\n         0.5438,  0.5102,  0.4770,  0.4443,  0.4118,  0.3795,  0.3470,  0.3141,\n         0.2805,  0.2455,  0.2084,  0.1672,  0.1174,  0.0292,  0.0000])\n\n\nWe use tqdm to show us a progress bar of our process.\n\nfrom tqdm.auto import tqdm\n\nWe are ready to test our ‚Äòunet‚Äô model on some noisy images. We will vary the noise level and see how well our model can denoise them. Then we will compare the predicted output with the original image.\nWe guide the predicted image to make it relevant and original to the prompt.\n\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    input = torch.cat([latents] * 2)\n    input = scheduler.scale_model_input(input, t)\n\n    # predict the noise residual\n    with torch.no_grad(): pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    pred_uncond, pred_text = pred.chunk(2)\n    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n    # compute the \"previous\" noisy sample\n    latents = scheduler.step(pred, t, latents).prev_sample\n\n\n\n\nWe have successfully created the images. We can then view our image as follows and finally we have an astronaut riding a cow. How cool is that? This is a fun way to experiment with image processing and create some hilarious results.\n\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\nimage = (image * 255).round().astype(\"uint8\")\nImage.fromarray(image)\n\n\n\n\nYou‚Äôve learned how to do prompt engineering, but what about more advanced techniques like negative prompts or image-to-image generation? If you‚Äôre feeling adventurous, you can try to implement them yourself. Or you can visit my github repo Github where I have all the code ready for you.\nI hope you enjoyed this tutorial and learned something new. Happy coding and keep me posted on your amazing projects."
  },
  {
    "objectID": "Publication.html",
    "href": "Publication.html",
    "title": "Blogsite",
    "section": "",
    "text": "Unsupervised Machine Learning For Early Faulty Device Detection\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/first/index.html",
    "href": "publications/first/index.html",
    "title": "Unsupervised Machine Learning For Early Faulty Device Detection",
    "section": "",
    "text": "Early faulty detection in air quality sensors is of increasing importance in ensuring reliable and accurate air quality readings. These sensors play a critical role in monitoring the pollution levels in the atmosphere however, due to their exposure to harsh weather, it‚Äôs natural that they will face wear and may require periodical servicing. The devices are rarely monitored in most cases, particularly in Uganda. The objective of this research is to build a model for the early detection of faulty sensors for replacement and repair. Using the unsupervised learning approaches in particular K-means and PCA; we train a predictive model and later evaluate it for accuracy using a test dataset. From the cluster analysis, the research identified 3 clusters that are; the health state cluster, nearing fault state cluster, and the faulty state cluster. The research discovered that there is no significant difference in adopting PCA as a preprocessing step in using the K-means algorithm. The research further recommended other clustering methods to be used to compare the accuracy score. It also recommended using more attributes for PCA to be effective.\nDownload paper on Makerere University Dissertation repository here or research gate here"
  }
]