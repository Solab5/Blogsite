[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, Iâ€™m Morris Twinomugisha . Iâ€™m a machine learning engineer who loves building data-science and developer tools ðŸ‘·ðŸ¼â€â™‚ï¸. Iâ€™â€™m as well a stint in management consulting."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nMakerere University, Kampala | UGANDA Bachelors Degree in Statistics | Aug 2019 - July 2022\nSt Maryâ€™s College Rushoroza | Kabale, UG Uganda Advanced Certificate of Education | Jan 2017 - Nov 2018"
  },
  {
    "objectID": "about.html#trainings",
    "href": "about.html#trainings",
    "title": "About",
    "section": "Trainings",
    "text": "Trainings\nPractical Deep Learning for coders | Fast.ai\nApplied DataScience | WorldQuant University"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nDeep Conclusions | Data Scientist | November 2022 - present\nSolab Associates | Data Analyst - Internship|"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogsite",
    "section": "",
    "text": "Have you ever wondered how websites recommend products for you!\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nMorris Twinomugisha\n\n\n\n\n\n\n  \n\n\n\n\nWhat is it really\n\n\n\n\n\n\n\nposts\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nMorris Twinomugisha\n\n\n\n\n\n\n  \n\n\n\n\nThe truth about Deep Learning\n\n\n\n\n\n\n\nposts\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2022\n\n\nMorris Twinomugisha\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "The truth about Deep Learning",
    "section": "",
    "text": "Awesome deep learning\n\nYeah, I mean it â€œAwesome!â€. Deep learning is the most awesome, exciting, and interesting stuff I feel every data scientist should brag about. When I started my data science journey, I thought Learning basic machine learning was all until I heard about things like neural networks, NLP, Computer vision, and so forth. I was so eager to learn how deep learning works. Along the journey, I realized every business or institution needs artificial intelligence in all their support systems other than staying in the comfort zone of traditional approaches to problem-solving and innovation.\n\nHowever, getting started can be hectic and overwhelming since resources are all over the web, and getting the right resources can be difficult unless you have someone already practicing it to guide you, else you can waste a lot of time without getting started.\nYou can imagine what I went through to get started. If you are passionate about deep learning and donâ€™t know where to start, or maybe youâ€™ve started but feel you arenâ€™t understanding anything from some random YouTube videos or resources. This short article is meant for you. Allow me to share with you a few of the facts and resources you need to know about deep learning maybe you could consider making a decision you will never regret today. Deep learning has proved to be the best machine learning technique in different areas all over the world including areas like Natural Language Processing (NLP), Computer Vision, Medicine, Biology, Image generation/enhancement, Recommendation systems, Playing games, Robotics, Other applications â€“ financial and logistical forecasting; text-to-speech; much more.\nMany experts will say that to get started with deep learning, you need a lot of math, lots of data, lots of expensive computers, or maybe a Ph.D.Â Trust me anyone irrespective of any field can practice deep learning. As long as you have an internet connection, you can use many online platforms to run deep learning codes on any computer. A commonly used one is a google Collab notebook. For math, simple high school math is sufficient for anyone to get started. The required basic Linear algebra and calculus can be understood along the way.\nIf youâ€™ve done some basic machine learning before, the approach to deep learning is almost the same. This however doesnâ€™t imply that deep learning is only for those that have done machine learning before, I have seen experts that have progressed with deep learning minus prior machine learning experience. A critique of basic machine learning is that it mainly involves dealing with quantitative data, unlike most deep learning problems.\nDeep learning problems are mostly texts from the web, sound, pictures, and videos among others thus there is always a need to train models that can enable transforming input data into numeric. This is because computers can only understand numbers.\nAll these applications stipulate why every business or institution needs artificial intelligence in all their support systems other than staying in the comfort zone of traditional approaches to problem-solving and innovation. If this has motivated you, you can check on the following resources which I believe can be a very mounting stone for anyone that would like to embark on a new journey of deep learning.\n\nlink to deep learning for coders(by Jeremy Howard and Sylvain Gugger)\nlink to MIT course for deep learning(MIT)\nLink to Coursera deep learning course(by Andrew Ng)"
  },
  {
    "objectID": "posts/blog2/index.html#what-is-it-really",
    "href": "posts/blog2/index.html#what-is-it-really",
    "title": "What is it really",
    "section": "WHAT IS IT REALLY?",
    "text": "WHAT IS IT REALLY?\nIn 2015 the idea of creating a computer system that could recognise images was considered so outrageously challenging that it was the basis of this XKCD joke:\n\nIn this tutorial we are going to create a computer vision model that can classify whether an image is a car, plane, train or bicycle. However this approach can work for any computer vision problem.\nBy the end of this tutorial, you will be able to implemenent an image classification model in just a few minutes, using entirely free resources!. We shall be using the fastai library A high framework library built on top of pytorch. Feel free to see the ducumentation here for more understanding.\nThe basic steps weâ€™ll take are:\n\nUse DuckDuckGo to search for images of car, plane, train or bicycle photos.\nFine-tune a pretrained neural network to recognise the groups\nTry running this model on a picture of a plane and see if it works."
  },
  {
    "objectID": "posts/blog2/index.html#step-1-download-images-of-plane-train-car-and-bicycles.",
    "href": "posts/blog2/index.html#step-1-download-images-of-plane-train-car-and-bicycles.",
    "title": "What is it really",
    "section": "Step 1: Download images of plane, train, car and bicycles.",
    "text": "Step 1: Download images of plane, train, car and bicycles.\nThe first thing we are going to do is to install the latest versions of fastai and duckduckgo_search since we shall be using it to search images on the internet.\n\n!pip install -Uqq fastai duckduckgo_search\n\nWe create a search_images function that we shall use to iterate on the different terms we shall be searching on the web.\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=100):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\nLetâ€™s start by searching for a plane photo and see what kind of result we get. Weâ€™ll start by getting URLs from a search:\n\n#NB: `search_images` depends on duckduckgo.com, which doesn't always return correct responses.\n#    If you get a JSON error, just try running it again (it may take a couple of tries).\nurls = search_images('plane photos', max_images=1)\nurls[0]\n\nSearching for 'plane photos'\n\n\n'http://airplanes.itsabouttravelling.com/wp-content/uploads/2020/02/c-fjzs-air-canada-boeing-777-300er-02-scaled.jpg'\n\n\nâ€¦and then download a URL and take a look at it:\n\nfrom fastdownload import download_url\ndest = 'plane.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\nNow letâ€™s do the same with â€œcar photosâ€ and see if it works as well:\n\ndownload_url(search_images('car photos', max_images=1)[0], 'car.jpg', show_progress=False)\nImage.open('car.jpg').to_thumb(256,256)\n\nSearching for 'car photos'\n\n\n\n\n\nOur searches seem to be giving reasonable results, so letâ€™s grab a few examples of each of â€œplaneâ€, â€œcarâ€, â€œtrainâ€ and â€œbicycleâ€ photos, and save each group of photos to a different folder:\n\nvehicles = 'car','train', 'bicycle', 'plane'\npath = Path('vehicles')\n\nfor o in vehicles:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'car photo'\nSearching for 'train photo'\nSearching for 'bicycle photo'\nSearching for 'plane photo'"
  },
  {
    "objectID": "posts/blog2/index.html#step-2-train-our-model",
    "href": "posts/blog2/index.html#step-2-train-our-model",
    "title": "What is it really",
    "section": "Step 2: Train our model",
    "text": "Step 2: Train our model\nSome photos might not download correctly which could cause our model training to fail, so weâ€™ll remove them:\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n17\n\n\nTo train a model, weâ€™ll need DataLoaders, which is an object that contains a training set (the images used to create a model) and a validation set (the images used to check the accuracy of a model â€“ not used during training). In fastai we can create that easily using a DataBlock, and view sample images from it:\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\nHere what each of the DataBlock parameters means:\nblocks=(ImageBlock, CategoryBlock),\nThe inputs to our model are images, and the outputs are categories (in this case, â€œplaneâ€, â€œcarâ€, â€œtrainâ€ or â€œbicycleâ€).\nget_items=get_image_files, \nTo find all the inputs to our model, run the get_image_files function (which returns a list of all image files in a path).\nsplitter=RandomSplitter(valid_pct=0.2, seed=42),\nSplit the data into training and validation sets randomly, using 20% of the data for the validation set.\nget_y=parent_label,\nThe labels (y values) is the name of the parent of each file (i.e.Â the name of the folder theyâ€™re in, which will be pane, car, train, or bicycle).\nitem_tfms=[Resize(192, method='squish')]\nBefore training, resize each image to 192x192 pixels by â€œsquishingâ€ it (as opposed to cropping it).\nNow weâ€™re ready to train our model. The fastest widely used computer vision model is resnet18. You can train this in a few minutes, even on a CPU! (On a GPU, it generally takes under 20 secondsâ€¦)\nfastai comes with a helpful fine_tune() method which automatically uses best practices for fine tuning a pre-trained model, so weâ€™ll use that.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.820234\n      0.014370\n      0.000000\n      00:09\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.037691\n      0.001882\n      0.000000\n      00:05\n    \n    \n      1\n      0.032922\n      0.003439\n      0.000000\n      00:04\n    \n    \n      2\n      0.022986\n      0.003907\n      0.000000\n      00:04\n    \n  \n\n\n\nGenerally when I run this I see 100% accuracy on the validation set (although it might vary a bit from run to run).\nâ€œFine-tuningâ€ a model means that weâ€™re starting with a model someone else has trained using some other dataset (called the pretrained model), and adjusting the weights a little bit so that the model learns to recognise your particular dataset. In this case, the pretrained model was trained to recognise photos in imagenet, and widely-used computer vision dataset with images covering 1000 categories).\nAnd another interesting thing to look at when dealing with categorical target variables is that you can run a confusion matrix which gives you a visual representation and a sense on what classes are hard to predict. For our case, there is no time any class is being predicted wrongly which means that our model is performing very well.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "posts/blog2/index.html#step-3-use-our-model",
    "href": "posts/blog2/index.html#step-3-use-our-model",
    "title": "What is it really",
    "section": "Step 3: Use our model",
    "text": "Step 3: Use our model\nLetâ€™s see what our model thinks about that car we downloaded at the start:\n\npred,_,probs = learn.predict(PILImage.create('car.jpg'))\n\n\n\n\n\n\n\n\n\nprint(f\"This is a: {pred}.\")\nprint(f\"Probability it's a car: {probs[1]:.4f}\")\n\nThis is a: car.\nProbability it's a car: 1.0000\n\n\nWe can then pass export to our learner to save our model as a pickel file. This means that any one now can use our model for making predictions\n\nlearn.export('model.pkl')\n\nGood job, resnet18. :)\nSo, as you see, in the space of a few years, creating computer vision classification models has gone from â€œso hard itâ€™s a jokeâ€ to â€œtrivially easy and freeâ€!\nItâ€™s not just in computer vision. Thanks to deep learning, computers can now do many things which seemed impossible just a few years ago, including creating amazing artworks, and explaining jokes. Itâ€™s moving so fast that even experts in the field have trouble predicting how itâ€™s going to impact society in the coming years.\nOne thing is clear â€“ itâ€™s important that we all do our best to understand this technology, because otherwise weâ€™ll get left behind!"
  },
  {
    "objectID": "posts/blog3/index.html#latent-factors",
    "href": "posts/blog3/index.html#latent-factors",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Latent factors",
    "text": "Latent factors\nThe idea behind collaborative filtering is latent factors. Latent factors make it possible for a model to tell what product you may like and these kinds of factors can be learned based on what other users like. > jargon: Latent factors are factors that are important for the prediction of the recommendations, but are not explicitly given to the model and instead learned."
  },
  {
    "objectID": "posts/blog3/index.html#learning-the-factors",
    "href": "posts/blog3/index.html#learning-the-factors",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Learning the factors",
    "text": "Learning the factors\nStep 1: Randomly initialize some parameters. The left matrix represents the embedding matrix for the the userid whereas the upper matrix is for the movieid and all these parameters are randomly initialised. For both cases, blue represents the biases. For ourcase each id (wether movieid or userid) the number of latent factors are the same. (for our case each id has 5 factors and the extra 6th (bias)). > jargon: Embedding matrix is what we multiply and embedding with, and in this case collaborative filtering problem is learned through training.\n\n\n\nlat\n\n\nStep 2: Calculate the predictions. Just think of a simple math problem \\(y=mx+c\\) where we have two changing parameters m and c where c is the bias term. Our bias works in the same way. We use dot product to multiply user factors and movies factors the add both the biases. This is how we obtain the values in white space. > jargon: Dot product is when you multiply the corresponding elements of two vectors and add them up\nStep 3: The next step is to calculate the loss. We can use any loss function as long as it can be optimized. Remember at each step we compare the predicted value and then compare it with the actual value then used use stochastic gradient descent to make the parameters as better as possible. In this tutorial we use mean squared error since it works pretty fine. Other loss functions like mean absolute error can work perfect as well"
  },
  {
    "objectID": "posts/blog3/index.html#creating-dataloaders",
    "href": "posts/blog3/index.html#creating-dataloaders",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\nRemember when we listed what was in our path, there were many things including the moivie names in u.item, I think it is better if we use this than using the ids. This makes our dataframe more human friendly. We can use pandas again to do this.\n\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nWe can then use merge to combine our movies dataframe with the ratings\n\nratings = ratings.merge(movies)\n\n\nratings.shape\n\n(100000, 5)\n\n\nAnd wow, our dataframe looks nice, We can then build a DataLoaders object from this table. By default, it takes the first column for the user, the second column for the item (here our movies), and the third column for the ratings. We need to change the value of item_name in our case to use the title instead of the IDs:\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      542\n      My Left Foot (1989)\n      4\n    \n    \n      1\n      422\n      Event Horizon (1997)\n      3\n    \n    \n      2\n      311\n      African Queen, The (1951)\n      4\n    \n    \n      3\n      595\n      Face/Off (1997)\n      4\n    \n    \n      4\n      617\n      Evil Dead II (1987)\n      1\n    \n    \n      5\n      158\n      Jurassic Park (1993)\n      5\n    \n    \n      6\n      836\n      Chasing Amy (1997)\n      3\n    \n    \n      7\n      474\n      Emma (1996)\n      3\n    \n    \n      8\n      466\n      Jackie Chan's First Strike (1996)\n      3\n    \n    \n      9\n      554\n      Scream (1996)\n      3\n    \n  \n\n\n\nTo represent collaborative filtering in PyTorch we canâ€™t just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:\n\nn_users  = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nWe can check the shape to ensure everything is running perfect\n\nuser_factors.shape, movie_factors.shape\n\n(torch.Size([944, 5]), torch.Size([1665, 5]))\n\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.\nFortunately, it turns out that we can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors."
  },
  {
    "objectID": "posts/blog3/index.html#collaborative-filtering-from-scratch",
    "href": "posts/blog3/index.html#collaborative-filtering-from-scratch",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Collaborative Filtering from Scratch",
    "text": "Collaborative Filtering from Scratch\nPyTorch already provides a Module class, which provides some basic foundations that we want to build on. We just add the name of this superclass after the name of the class that we are defining.\nThe final thing that you need to know to create a new PyTorch module is that when your module is called, PyTorch will call a method in your class called forward, and will pass along to that any parameters that are included in the call. Here is the class defining our dot product model:\nIf you havenâ€™t seen object-oriented programming before, it can be a little bit intimidating but then donâ€™t worry, its a few lines of code and can be easy to read when you do it line by line. But dont hesitate to google some tutorial and have a sense how object oriented programming works.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n\nWe now have our architecture ready, Letâ€™s train and see how everything goes\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.958848\n      0.956008\n      00:10\n    \n    \n      1\n      0.872096\n      0.876626\n      00:10\n    \n    \n      2\n      0.747695\n      0.839320\n      00:10\n    \n    \n      3\n      0.600863\n      0.822649\n      00:10\n    \n    \n      4\n      0.487104\n      0.822984\n      00:10\n    \n  \n\n\n\nAnd wow, this looks fine with just 5 epochs. You can try iterating more and changing the learning rate to see wether you can make this better. The wd stands for weight decay but most deeplearning practioners call it L2 regularization. So itâ€™s better to understand such jargon since thats what most reseach papers use.\nActually we used pytorchâ€™s embedding previously to initialize our parameters, we can decide not to do that and build our own. Letâ€™s do that. We begin by creating a function that takes size as an argument and then returns a tensor if randomly initialized parameters.\n\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\nWe can then pass in the function instead of pytorchâ€™s embedding.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n\nLetâ€™s train again.\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.944513\n      0.958574\n      00:11\n    \n    \n      1\n      0.847001\n      0.873356\n      00:10\n    \n    \n      2\n      0.736139\n      0.838051\n      00:11\n    \n    \n      3\n      0.584478\n      0.821091\n      00:11\n    \n    \n      4\n      0.484529\n      0.821726\n      00:10\n    \n  \n\n\n\nAnd wow, it even looks a little bit better than using pytorchâ€™s embedding. This shows you the power of doing things from scratch."
  },
  {
    "objectID": "posts/blog3/index.html#interpreting-embeddings-and-biases",
    "href": "posts/blog3/index.html#interpreting-embeddings-and-biases",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\nTo be an expert deep learning practioner, you need to be able to interprete models, there are some insights you canâ€™t get from the data till you first run the models. Forexample letâ€™s try interpreting the bias. We are using argsort to sort the values in ascending order and return the first five indices of those values. When we index them onto the title class, we get their names and print them.\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Robocop 3 (1993)',\n 'Home Alone 3 (1997)',\n 'Mortal Kombat: Annihilation (1997)']\n\n\nThe interpretation for this is that letâ€™s say Robocop in this list is an action movie (I am not sure of it being an action movie). People that like movies may not like it as well. This implys that this particular movie wonâ€™t be recommended to users.\nWe can do the same for those movies with a high bias by passing True to descending. This prints for us movies that are liked. Forexample say Titanic, a user might like it even if he or she doesnâ€™t like romantic movies. So such a movie will be highly recommended for users to watch.\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n 'Shawshank Redemption, The (1994)',\n \"Schindler's List (1993)\",\n 'Titanic (1997)',\n 'Silence of the Lambs, The (1991)']\n\n\nWe can also do an interpretation on the embeddings, we have 50 factors and representing them on a 2D graph isnot possible. We can use PCA to just reduce the factors two just two dimensions. Surprisingly similars will tend to be clustered near each other, this is something that we didnt program. We didnt even do any feature engineering but the model does this for us. And as I said, these are insights you cannot get from the data untill you run the models.\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\nAll this that weâ€™ve written from from scratch can be done using a fastaiâ€™s framwork collab_learner. Letâ€™s try it and see whether we get similar results."
  },
  {
    "objectID": "posts/blog3/index.html#using-fastai-collab-learner",
    "href": "posts/blog3/index.html#using-fastai-collab-learner",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Using fastai Collab-Learner",
    "text": "Using fastai Collab-Learner\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.962636\n      0.950697\n      00:10\n    \n    \n      1\n      0.851842\n      0.885249\n      00:12\n    \n    \n      2\n      0.757142\n      0.839390\n      00:11\n    \n    \n      3\n      0.572357\n      0.825587\n      00:11\n    \n    \n      4\n      0.490504\n      0.826604\n      00:11\n    \n  \n\n\n\nWe can also look into our model,\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nAnd surprisingly, when we check the movie with maximum bias weights. We get the same as our model built from scratch.\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n 'Silence of the Lambs, The (1991)',\n 'Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Star Wars (1977)']\n\n\nThese days the cool kids are using deep-learning. Letâ€™s try it out and see how it works as well."
  },
  {
    "objectID": "posts/blog3/index.html#deep-learning",
    "href": "posts/blog3/index.html#deep-learning",
    "title": "Have you ever wondered how websites recommend products for you!",
    "section": "Deep learning",
    "text": "Deep learning\nFirstai has a function get_emb_sz that recommends for as sizes to use for the factors, however you can use different sizes other than this.\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nAnd like other deep learning frameworks, we need a sequential layer That has both linear and ReLU activations that we pass in our data. In the forward function, we are concatenating the embeddngs for both the users and the movies and this is actually what we pass to the sequential layer. Then pytorch will calculate gradients for us and stochastic gradient descent as well.\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n\nWe can train as usual.\n\nmodel = CollabNN(*embs)\n\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.921936\n      0.952098\n      00:12\n    \n    \n      1\n      0.868925\n      0.927091\n      00:10\n    \n    \n      2\n      0.885512\n      0.885665\n      00:11\n    \n    \n      3\n      0.850595\n      0.873478\n      00:11\n    \n    \n      4\n      0.761019\n      0.877106\n      00:11\n    \n  \n\n\n\nSurprisingly all our previous models beat the deep learning approach. This shows us why deep-learning shouldnâ€™t always be the first priority. At-times simpler models can work just perfect.\nThank you, Hope this tutorial will be helpful."
  },
  {
    "objectID": "Publication.html",
    "href": "Publication.html",
    "title": "Blogsite",
    "section": "",
    "text": "Unsupervised Machine Learning For Early Faulty Device Detection\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/first/index.html",
    "href": "publications/first/index.html",
    "title": "Unsupervised Machine Learning For Early Faulty Device Detection",
    "section": "",
    "text": "Early faulty detection in air quality sensors is of increasing importance in ensuring reliable and accurate air quality readings. These sensors play a critical role in monitoring the pollution levels in the atmosphere however, due to their exposure to harsh weather, itâ€™s natural that they will face wear and may require periodical servicing. The devices are rarely monitored in most cases, particularly in Uganda. The objective of this research is to build a model for the early detection of faulty sensors for replacement and repair. Using the unsupervised learning approaches in particular K-means and PCA; we train a predictive model and later evaluate it for accuracy using a test dataset. From the cluster analysis, the research identified 3 clusters that are; the health state cluster, nearing fault state cluster, and the faulty state cluster. The research discovered that there is no significant difference in adopting PCA as a preprocessing step in using the K-means algorithm. The research further recommended other clustering methods to be used to compare the accuracy score. It also recommended using more attributes for PCA to be effective.\nDownload paper on Makerere University Dissertation repository here or research gate here"
  }
]